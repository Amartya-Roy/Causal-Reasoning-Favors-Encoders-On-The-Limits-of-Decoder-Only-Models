{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35212012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from llmclient import LLMClient, get_llm_response\n",
    "#import pandas as pd\n",
    "\n",
    "def retrieve(prompt: list, llm: LLMClient, is_reasoning=False, has_think=False):\n",
    "    \"\"\" Retrieve a response from an LLM. Useful in APIs where there is rate limitations.\n",
    "    ---\n",
    "    Params:\n",
    "    prompt: list: a list of inputs (i.e., [{\"role\": \"user\", \"content\": ...}])\n",
    "    llm: the LLMClient object\n",
    "    is_reasoning: is it a reasoning model? -- note, GPT-5 does _not_ qualify for this because it does not return its CoT\n",
    "    has_think: does it have a </think> token?\n",
    "    \"\"\"\n",
    "    response = None\n",
    "    tries = 0\n",
    "    while True:\n",
    "        if response is not None: break\n",
    "        if tries > 5: break\n",
    "        try:\n",
    "            response = get_llm_response(llm, prompt)\n",
    "            response = response.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            if response == \"\":\n",
    "                response = None\n",
    "                tries += 1\n",
    "            else:\n",
    "                if is_reasoning:\n",
    "                    if has_think:\n",
    "                        response = response.split(\"</think>\")[-1].strip()\n",
    "                    else:\n",
    "                        response = \"{\" + response.split(\"{\")[-1].strip()\n",
    "                response = json.loads(response.strip())\n",
    "                response[\"Label\"] = int(response[\"Label\"])\n",
    "        except:\n",
    "            response = None\n",
    "            tries += 1\n",
    "    failed = False\n",
    "    if response is None:\n",
    "        failed = True\n",
    "        response = {\"Label\": 0}\n",
    "    return response, failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_prompt(question: str, exemplars: list):\n",
    "    \"\"\" Retrieve the evaluation prompt.\n",
    "    ---\n",
    "    Params: \n",
    "    question: str: the full query (a [IMPLY] b [AND] c ... including the query)\n",
    "    exemplars: list: a list of exemplars. May be an empty list.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are evaluating a subset of first-order logic. \n",
    "In this subset, conjunctions are given by [AND], implications by [IMPLY], and separations between clauses as [PERIOD]\n",
    "You will be given Facts, and Rules. Based on these, determine the truth value of the Query.\n",
    "Your final answer should be 0 (if the Query is false) or 1 (if true).\n",
    "\n",
    "Give your answer in JSON with the following schema:\n",
    "{{\n",
    "\"Label\" (int): The label from the criterion. Only use the numbers 0 or 1.\n",
    "}}\n",
    "Only use the key \"Label\".\n",
    "\"\"\"\n",
    "    exemplars = []\n",
    "    for e in exemplars:\n",
    "        t = e[\"Question\"]\n",
    "        label = e[\"Response\"]\n",
    "        exemplars.append({\"role\": \"user\", \"content\": f\"Question: {t}\"})\n",
    "        exemplars.append({\"role\": \"assistant\", \"content\": '{\"Label\": '  + label + '}'})\n",
    "    user_prompt = f\"Question: {question}\"\n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    prompt += exemplars\n",
    "    prompt += [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NL\n",
    "dataset = json.load(open(\"model_responses_final.json\", \"r\", encoding=\"utf-8\"))\n",
    "exemplars = json.load(open(\"exemplars.json\", \"r\", encoding=\"utf-8\"))[:5]\n",
    "\n",
    "# Non-NL\n",
    "dataset = json.load(open(\"test_gibberish.json\", \"r\", encoding=\"utf-8\"))\n",
    "exemplars = json.load(open(\"exemplars_gibberish.json\", \"r\", encoding=\"utf-8\"))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-5-reasoning\"\n",
    "MODEL = \"anthropic-claude-opus-4-1\"\n",
    "params = {\"max_tokens\": 6000,}\n",
    "\n",
    "MODEL = \"qwen-3-17b\"\n",
    "params = {\"max_completion_tokens\": 4500,}\n",
    "\n",
    "MODEL = \"qwen-25-vl7b\"\n",
    "MODEL = \"gpt-41-shortco-2025-04-14\"\n",
    "params = {\"max_tokens\": 128}\n",
    "\n",
    "llm = LLMClient(params, MODEL)\n",
    "model_name = MODEL\n",
    "\n",
    "is_zero_shot = True\n",
    "outputs = dataset # For checkpointing\n",
    "mname = model_name + \"_five_shot\"\n",
    "if is_zero_shot:\n",
    "    exemplars = []\n",
    "    mname = model_name + \"_zero_shot\"\n",
    "\n",
    "fails = 0\n",
    "optimise_tokens = \"gpt-5\" in MODEL.lower() or \"claude\" in MODEL.lower() # Some LLMs are too slow\n",
    "is_reasoning = False # Claude, Qwen-3 (not GPT-5)\n",
    "has_think = False # Does it have a </think> output?\n",
    "\n",
    "for i in tqdm(range(len(outputs))):\n",
    "    entry = outputs[i]\n",
    "    query = entry[\"Question\"]\n",
    "    prompt = get_eval_prompt(query, exemplars)\n",
    "\n",
    "    if optimise_tokens:\n",
    "        if entry['Depth'] > 25:\n",
    "            params = {\"max_completion_tokens\": 9000}\n",
    "        if entry['Depth'] <= 25:\n",
    "            params =  {\"max_completion_tokens\": 7000}\n",
    "        if entry['Depth'] <= 18:\n",
    "            params = {\"max_completion_tokens\": 5000}\n",
    "    response, failed = retrieve(prompt, llm, is_reasoning=is_reasoning, has_think=has_think)\n",
    "    if failed: fails += 1\n",
    "    if \"Scores\" not in entry: entry[\"Scores\"] = {}\n",
    "    entry[\"Scores\"][mname] = response[\"Label\"]\n",
    "\n",
    "    # We checkpoint in case of issues\n",
    "    with open(\"tmp_scores.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "\n",
    "print(fails, round(fails*100/len(outputs), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86831e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs =  [json.loads(l) for l in open(\"tmp_scores.json\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "with open(\"model_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(outputs, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
